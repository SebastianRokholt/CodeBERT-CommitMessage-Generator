{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM9A6cjr9Ekv"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nf02JDCYg3vd",
    "outputId": "22716322-29ee-4a94-9a5a-cd7b83cb9530"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/graykode/commit-autosuggestions.git\n",
    "#%cd commit-autosuggestions\n",
    "# !pip install commit\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v63CrlaXatVF",
    "outputId": "f3cd0396-c212-414f-845c-5f98c72253c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 18:10:23.781804: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-05 18:10:23.971012: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-05 18:10:24.896110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2022-11-05 18:10:24.896174: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2022-11-05 18:10:24.896181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Utility stuff\n",
    "import os\n",
    "import easydict \n",
    "import whatthepatch\n",
    "\n",
    "# Deep learning stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from transformers import (RobertaConfig, RobertaTokenizer)\n",
    "from tqdm import tqdm\n",
    "from commit.model import Seq2Seq\n",
    "from commit.utils import convert_examples_to_features\n",
    "from commit.model.diff_roberta import RobertaModel\n",
    "\n",
    "# Constants\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJOBGc9o9N_q"
   },
   "source": [
    "### Download Greykode's CodeBERT fine-tuned decoder weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcDZ6FeAhB6Y",
    "outputId": "059ec1d0-c4bf-49e0-fd86-825dba8b55e9"
   },
   "outputs": [],
   "source": [
    "# ADD_MODEL='1YrkwfM-0VBCJaa9NYaXUQPODdGPsmQY4'\n",
    "# DIFF_MODEL='1--gcVVix92_Fp75A-mWH0pJS0ahlni5m'\n",
    "\n",
    "# !pip install gdown \\\n",
    "#     && mkdir -p weight/added \\\n",
    "#     && mkdir -p weight/diff \\\n",
    "#     && gdown \"https://drive.google.com/uc?id=$ADD_MODEL\" -O weight/added/pytorch_model.bin \\\n",
    "#     && gdown \"https://drive.google.com/uc?id=$DIFF_MODEL\" -O weight/diff/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_u1osYJ86B0"
   },
   "source": [
    "### Running predictions on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6FGW8SsKjeGR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?   False\n"
     ]
    }
   ],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 idx,\n",
    "                 added,\n",
    "                 deleted,\n",
    "                 target,\n",
    "                 ):\n",
    "        self.idx = idx\n",
    "        self.added = added\n",
    "        self.deleted = deleted\n",
    "        self.target = target\n",
    "\n",
    "def get_model(model_class, config, tokenizer, mode):\n",
    "    encoder = model_class(config=config)\n",
    "    decoder_layer = nn.TransformerDecoderLayer(\n",
    "        d_model=config.hidden_size, nhead=config.num_attention_heads\n",
    "    )\n",
    "    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "    model = Seq2Seq(encoder=encoder, decoder=decoder, config=config,\n",
    "            beam_size=args.beam_size, max_length=args.max_target_length,\n",
    "            sos_id=tokenizer.cls_token_id, eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "    assert args.load_model_path\n",
    "    print(\"model path: \", os.path.join(args.load_model_path, mode, 'pytorch_model.bin'))\n",
    "    assert os.path.exists(os.path.join(args.load_model_path, mode, 'pytorch_model.bin'))\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            os.path.join(args.load_model_path, mode, 'pytorch_model.bin'),\n",
    "            map_location=torch.device('cpu')\n",
    "        ),\n",
    "        strict=False\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_features(examples):\n",
    "    features = convert_examples_to_features(examples, args.tokenizer, args, stage='test')\n",
    "    all_source_ids = torch.tensor(\n",
    "        [f.source_ids[:args.max_source_length] for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_source_mask = torch.tensor(\n",
    "        [f.source_mask[:args.max_source_length] for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_patch_ids = torch.tensor(\n",
    "        [f.patch_ids[:args.max_source_length] for f in features], dtype=torch.long\n",
    "    )\n",
    "    return TensorDataset(all_source_ids, all_source_mask, all_patch_ids)\n",
    "\n",
    "def inference(model, data):\n",
    "    \"\"\"\n",
    "    :data: A torch.utils.data.dataset.TensorDataset object\n",
    "    \"\"\"\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(data)\n",
    "    eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=len(data))\n",
    "\n",
    "    model.eval()\n",
    "    p = []\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader)):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        source_ids, source_mask, patch_ids = batch\n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids=source_ids, source_mask=source_mask, patch_ids=patch_ids)\n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = args.tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    return p\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'load_model_path': 'weight/', \n",
    "    'model_type': 'roberta',\n",
    "    'config_name' : 'microsoft/codebert-base',\n",
    "    'tokenizer_name' : 'microsoft/codebert-base',\n",
    "    'max_source_length' : 512,\n",
    "    'max_target_length' : 128,\n",
    "    'beam_size' : 10,\n",
    "    'do_lower_case' : False,\n",
    "    'device' : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "})\n",
    "\n",
    "print(\"Is GPU available?  \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZFdp9aL9ior"
   },
   "source": [
    "Building the PL-NL model with the fine-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "zsISSK8oQBfD",
    "outputId": "5f0c9099-ef46-4daa-feef-43fca2ee5d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path:  weight/added/pytorch_model.bin\n",
      "model path:  weight/diff/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name)\n",
    "args.tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name, do_lower_case=args.do_lower_case)\n",
    "\n",
    "# Build model\n",
    "args.added_model = get_model(model_class=model_class, config=config,\n",
    "                        tokenizer=args.tokenizer, mode='added').to(args.device)\n",
    "args.diff_model = get_model(model_class=model_class, config=config,\n",
    "                        tokenizer=args.tokenizer, mode='diff').to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dtAhq1V99cY"
   },
   "source": [
    "Running prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffmessage = \"\"\"\n",
    "diff --git a/newfile.py b/newfile.py\n",
    "new file mode 100644\n",
    "index 0000000..cbb72b8\n",
    "--- /dev/null\n",
    "+++ b/newfile.py\n",
    "@@ -0,0 +1,9 @@\n",
    "+#!/usr/bin/env python3\n",
    "+\n",
    "+import numpy as np\n",
    "+def multiply_vectors(v1, v2):\n",
    "+    return np.dot(v1, v2)\n",
    "\"\"\"\n",
    "\n",
    "diffmessage2 = \"\"\"\n",
    "diff --git a/test.py b/test.py\n",
    "index d13f441..1b1b82a 100644\n",
    "--- a/test.py\n",
    "+++ b/test.py\n",
    "@@ -1,6 +1,3 @@\n",
    "\n",
    "-import torch\n",
    "-import argparse\n",
    "-import numpy\n",
    "-import sklearn\n",
    "+import matplotlib.pyplot as plt\n",
    " def add(a, b):\n",
    "     return a + b\n",
    "\"\"\"\n",
    "\n",
    "diffmessage3 = \"\"\"\n",
    "diff --git a/test.py b/test.py\n",
    "new file mode 100644\n",
    "index 0000000..d13f441\n",
    "--- /dev/null\n",
    "+++ b/test.py\n",
    "@@ -0,0 +1,6 @@\n",
    "+\n",
    "+import torch\n",
    "+import argparse\n",
    "+\n",
    "+def add(a, b):\n",
    "+    return a + b\n",
    "\"\"\"\n",
    "\n",
    "diffmessage4 = \"\"\"\n",
    "diff --git a/newfile.py b/newfile.py\n",
    "new file mode 100644\n",
    "index 0000000..8724a42\n",
    "--- /dev/null\n",
    "+++ b/newfile.py\n",
    "@@ -0,0 +1,5 @@\n",
    "+#!usr/bin/env python3\n",
    "+\n",
    "+# Gets the url\n",
    "+def get_url(domain, path):\n",
    "+    return domain + \"/\" + path\n",
    "diff --git a/ngrok.conf b/ngrok.conf\n",
    "new file mode 100644\n",
    "index 0000000..8e50d0f\n",
    "--- /dev/null\n",
    "+++ b/ngrok.conf\n",
    "@@ -0,0 +1,11 @@\n",
    "+\n",
    "+authtoken: 1kskZgJ8KpCRvYnzSF63AcodvBr_4RMXxFo4Sa2qLrRaKjhJW\n",
    "+region: jp\n",
    "+console_ui: False\n",
    "+tunnels:\n",
    "+  input:\n",
    "+    addr: 5000\n",
    "+    proto: http    \n",
    "+  output:\n",
    "+    addr: 5000\n",
    "+    proto: http\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0bq--xO2D6vl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '!/', 'usr', '/', 'bin', '/', 'env', 'Ġpython', '3', 'Ġimport', 'Ġn', 'umpy', 'Ġas', 'Ġnp', 'Ġdef', 'Ġmultiply', '_', 've', 'ctors', '(', 'v', '1', ',', 'Ġv', '2', '):', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġnp', '.', 'dot', '(', 'v', '1', ',', 'Ġv', '2', ')']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/1 [00:00<?, ?it/s]/home/srokholt/anaconda3/envs/inf367-env/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autogenerated commit message: \n",
      " ['Multiply two vectors .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the git diff with whatthepatch package\n",
    "# Retrieve changes (added and deleted lines of code)\n",
    "for idx, example in enumerate(whatthepatch.parse_patch(diffmessage)):\n",
    "    if not example.changes:\n",
    "        print(f\"no changes in {idx}\")\n",
    "        continue\n",
    "\n",
    "    isadded, isdeleted = False, False\n",
    "    added, deleted = [], []\n",
    "\n",
    "    # Determine if the line is added or deleted and add it to the corresponding list\n",
    "    for change in example.changes:\n",
    "        if change.old == None and change.new != None and change.line != \"\":\n",
    "            added.append(change.line)\n",
    "            isadded = True\n",
    "        elif change.old != None and change.new == None and change.line != \"\":\n",
    "            deleted.append(change.line)\n",
    "            isdeleted = True\n",
    "    \n",
    "    # Tokenization\n",
    "    added_tokens = args.tokenizer.tokenize(\" \".join(added))\n",
    "    deleted_tokens = args.tokenizer.tokenize(\" \".join(deleted))\n",
    "    print(added_tokens)\n",
    "\n",
    "    # If code was only added to the file, we can run inference with the added model\n",
    "    if isadded and not isdeleted:        \n",
    "        # Create a numerical vector representation\n",
    "        testsample = [Example(idx, added_tokens, deleted_tokens, target=None)]\n",
    "        sampledata = get_features(testsample)\n",
    "        \n",
    "        # Generate a commit message\n",
    "        message = inference(model=args.added_model, data=sampledata)\n",
    "        print(\"Autogenerated commit message: \\n\", message)\n",
    "    \n",
    "    # If code was deleted in the changed file, we need to run inference with the diff model\n",
    "    else: \n",
    "        # Create a numerical vector representation\n",
    "        testsample = [Example(idx, added_tokens, deleted_tokens, target=None)]\n",
    "        sampledata = get_features(testsample)\n",
    "        \n",
    "        # Generate a commit message\n",
    "        message = inference(model=args.diff_model, data=sampledata)\n",
    "        print(\"Autogenerated commit message: \\n\", message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('inf367-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "db245c8b99c89133456aa2a54e87de631a0c867b0c5d38751bca1e49b65a1613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
